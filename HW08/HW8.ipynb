{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3"},"metadata":{"interpreter":{"hash":"01a67b10546e3f87b50d3a06e486977983264703464e3ef8385e083c602a0edb"}},"colab":{"name":"HW8.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"5EsL4_JKqjmG"},"source":["# A denoising autoencoder for CIFAR dataset(s)\n","# Reference: https://codahead.com/blog/a-denoising-autoencoder-for-cifar-datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-1FdwyzrTH3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621585360768,"user_tz":-480,"elapsed":2398,"user":{"displayName":"曾仲宇","photoUrl":"","userId":"09975204903538384010"}},"outputId":"6d28d04f-af0b-4bfc-cffd-0f06b5c1161c"},"source":["import torch\n","try:\n","    # Get GPU name, check if it's K80\n","    GPU_name = torch.cuda.get_device_name()\n","    if GPU_name[-3:] == \"K80\":\n","        print(\"Get K80! :'( RESTART!\")\n","        exit()  # Restart the session\n","    else:\n","        print(\"Your GPU is {}!\".format(GPU_name))\n","        print(\"Great! Keep going~\")\n","except RuntimeError as e:\n","    if e.args == (\"No CUDA GPUs are available\",):\n","        print(\"You are training with CPU! \"\n","              \"Please restart!\")\n","        exit()  # Restart the session\n","    else:\n","        print(\"What's wrong here?\")\n","        print(\"Error message: \\n\", e)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your GPU is Tesla P100-PCIE-16GB!\n","Great! Keep going~\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dfqPBGxoWpkC","executionInfo":{"status":"ok","timestamp":1621585424613,"user_tz":-480,"elapsed":63846,"user":{"displayName":"曾仲宇","photoUrl":"","userId":"09975204903538384010"}},"outputId":"aba4d8e7-1f13-442c-d905-45a42d863e6c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","\n","# your workspace in your drive\n","workspace = 'Course_ML2021Spring/HW08'\n","\n","\n","try:\n","  os.chdir(os.path.join('/content/gdrive/My Drive/', workspace))\n","except:\n","  os.mkdir(os.path.join('/content/gdrive/My Drive/', workspace))\n","  os.chdir(os.path.join('/content/gdrive/My Drive/', workspace))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fOQxPlY5qjmJ"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Conv2D, Input, Dense, Reshape, Conv2DTranspose,\\\n","   Activation, BatchNormalization, ReLU, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5moMk-QqjmK"},"source":["train_data_clean = np.load('data-bin/trainingset.npy', allow_pickle=True).astype('float32') / 255.\n","test_data_clean = np.load('data-bin/testingset.npy', allow_pickle=True).astype('float32') / 255.\n","# Garbage Collector - use it like gc.collect()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHiaMTMYqjmM"},"source":["# add_noise_and_clip_data\n","train_data_noisy = np.clip(train_data_clean + np.random.normal(loc=0.0, scale=0.1, size=train_data_clean.shape), 0., 1.)\n","# test_data_noisy = add_noise_and_clip_data(test_data_clean)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ab5hwCD2qjmN"},"source":["idx = 1\n","import matplotlib.pyplot as plt\n","plt.subplot(1,2,1)\n","plt.imshow(train_data_clean[idx])\n","plt.title('Original image')\n","plt.subplot(1,2,2)\n","plt.imshow(train_data_noisy[idx])\n","plt.title('Image with noise')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtFA6kq7qjmN"},"source":["def conv_block(x, filters, kernel_size, strides=2):\n","   x = Conv2D(filters=filters,\n","              kernel_size=kernel_size,\n","              strides=strides,\n","              padding='same')(x)\n","   x = BatchNormalization()(x)\n","   x = ReLU()(x)\n","   return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXDByJloqjmN"},"source":["def deconv_block(x, filters, kernel_size):\n","   x = Conv2DTranspose(filters=filters,\n","                       kernel_size=kernel_size,\n","                       strides=2,\n","                       padding='same')(x)\n","   x = BatchNormalization()(x)\n","   x = ReLU()(x)\n","   return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ckm0KgeVqjmO"},"source":["def denoising_autoencoder():\n","    dae_inputs = Input(shape=(64, 64, 3), name='dae_input')\n","    conv_block1 = conv_block(dae_inputs, 64, 3)\n","    conv_block2 = conv_block(conv_block1, 128, 3)\n","    conv_block3 = conv_block(conv_block2, 256, 3)\n","    conv_block4 = conv_block(conv_block3, 512, 3)\n","    conv_block5 = conv_block(conv_block4, 512, 3, 1)\n","    \n","    deconv_block1 = deconv_block(conv_block5, 512, 3)\n","    merge1 = Concatenate()([deconv_block1, conv_block3])\n","    deconv_block2 = deconv_block(merge1, 256, 3)\n","    merge2 = Concatenate()([deconv_block2, conv_block2])\n","    deconv_block3 = deconv_block(merge2, 128, 3)\n","    merge3 = Concatenate()([deconv_block3, conv_block1])\n","    deconv_block4 = deconv_block(merge3, 64, 3)\n","    \n","    final_deconv = Conv2DTranspose(filters=3,kernel_size=3,padding='same')(deconv_block4)\n","    \n","    dae_outputs = Activation('sigmoid', name='dae_output')(final_deconv)\n","    \n","    return Model(dae_inputs, dae_outputs, name='dae')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3zGC1HnqjmO"},"source":["dae = denoising_autoencoder()\n","dae.compile(loss='mse', optimizer='adam')\n","epochs = 30\n","\n","checkpoint = ModelCheckpoint(filepath = './TFoutput/{epoch:04d}.h5', verbose=1, save_best_only=False, save_weights_only=True)\n","\n","dae.fit(train_data_noisy,\n","       train_data_clean,\n","    #    validation_data=(train_data_noisy, train_data_clean),\n","       epochs=epochs,\n","       batch_size=128,\n","       callbacks=[checkpoint])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQAHT9_-qjmP"},"source":["#Inference to do anomoly detection\n","dae.load_weights('./TFoutput/0030.h5')\n","test_data_denoised = dae.predict(test_data_noisy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wUi8_TqqjmP"},"source":["test_data_denoised.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSTp8vuWqjmP"},"source":["# MSE\n","anomality =  np.square(np.subtract(test_data_clean, test_data_denoised)).sum(-1)\n","\n","# anomality = losses.MSE(test_data_clean ,test_data_denoised)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YswvfNW-qjmQ"},"source":["anomalityScore = anomality.sum(axis = (1,2))\n","anomalityScore = np.sqrt(anomalityScore)\n","AnomalityScore = np.expand_dims(anomalityScore , axis=-1)\n","AnomalityScore.shape\n","AnomalityScore[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAK9GP7fqjmR"},"source":["import pandas as pd\n","out_file = 'PREDICTION_FILE0.csv'\n","df = pd.DataFrame(AnomalityScore, columns=['Predicted'])\n","df.to_csv(out_file, index_label = 'Id')"],"execution_count":null,"outputs":[]}]}