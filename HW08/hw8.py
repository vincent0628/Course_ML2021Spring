# -*- coding: utf-8 -*-
"""HW8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NpU9JdsZ8PBQNB-fnROjHRW08dRTxdIX
"""

# A denoising autoencoder for CIFAR dataset(s)
# Reference: https://codahead.com/blog/a-denoising-autoencoder-for-cifar-datasets



import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Conv2D, Input, Dense, Reshape, Conv2DTranspose,\
   Activation, BatchNormalization, ReLU, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint

train_data_clean = np.load('data-bin/trainingset.npy', allow_pickle=True).astype('float32') / 255.
test_data_clean = np.load('data-bin/testingset.npy', allow_pickle=True).astype('float32') / 255.
# Garbage Collector - use it like gc.collect()

# add_noise_and_clip_data
train_data_noisy = np.clip(train_data_clean + np.random.normal(loc=0.0, scale=0.1, size=train_data_clean.shape), 0., 1.)
# test_data_noisy = add_noise_and_clip_data(test_data_clean)

idx = 1
import matplotlib.pyplot as plt
plt.subplot(1,2,1)
plt.imshow(train_data_clean[idx])
plt.title('Original image')
plt.subplot(1,2,2)
plt.imshow(train_data_noisy[idx])
plt.title('Image with noise')
plt.show()

def conv_block(x, filters, kernel_size, strides=2):
   x = Conv2D(filters=filters,
              kernel_size=kernel_size,
              strides=strides,
              padding='same')(x)
   x = BatchNormalization()(x)
   x = ReLU()(x)
   return x

def deconv_block(x, filters, kernel_size):
   x = Conv2DTranspose(filters=filters,
                       kernel_size=kernel_size,
                       strides=2,
                       padding='same')(x)
   x = BatchNormalization()(x)
   x = ReLU()(x)
   return x

def denoising_autoencoder():
    dae_inputs = Input(shape=(64, 64, 3), name='dae_input')
    conv_block1 = conv_block(dae_inputs, 64, 3)
    conv_block2 = conv_block(conv_block1, 128, 3)
    conv_block3 = conv_block(conv_block2, 256, 3)
    conv_block4 = conv_block(conv_block3, 512, 3)
    conv_block5 = conv_block(conv_block4, 512, 3, 1)
    
    deconv_block1 = deconv_block(conv_block5, 512, 3)
    merge1 = Concatenate()([deconv_block1, conv_block3])
    deconv_block2 = deconv_block(merge1, 256, 3)
    merge2 = Concatenate()([deconv_block2, conv_block2])
    deconv_block3 = deconv_block(merge2, 128, 3)
    merge3 = Concatenate()([deconv_block3, conv_block1])
    deconv_block4 = deconv_block(merge3, 64, 3)
    
    final_deconv = Conv2DTranspose(filters=3,kernel_size=3,padding='same')(deconv_block4)
    
    dae_outputs = Activation('sigmoid', name='dae_output')(final_deconv)
    
    return Model(dae_inputs, dae_outputs, name='dae')

dae = denoising_autoencoder()
dae.compile(loss='mse', optimizer='adam')


checkpoint = ModelCheckpoint(filepath = './TFoutput/{epoch:04d}.h5', verbose=1, save_best_only=False, save_weights_only=True)

dae.fit(train_data_noisy,
       train_data_clean,
    #    validation_data=(train_data_noisy, train_data_clean),
       epochs=40,
       batch_size=128,
       callbacks=[checkpoint])

#Inference to do anomoly detection
dae.load_weights('./TFoutput/0040.h5')
test_data_denoised = dae.predict(test_data_noisy)

test_data_denoised.shape

# MSE
anomality =  np.square(np.subtract(test_data_clean, test_data_denoised)).sum(-1)

# anomality = losses.MSE(test_data_clean ,test_data_denoised)

anomalityScore = anomality.sum(axis = (1,2))
anomalityScore = np.sqrt(anomalityScore)
AnomalityScore = np.expand_dims(anomalityScore , axis=-1)
AnomalityScore.shape
AnomalityScore[:10]

import pandas as pd
out_file = 'PREDICTION_FILE0.csv'
df = pd.DataFrame(AnomalityScore, columns=['Predicted'])
df.to_csv(out_file, index_label = 'Id')